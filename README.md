<div align="center">

# ğŸŒŠ Flow

### Modern C++ Asynchronous Execution Library

![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)
[![C++](https://img.shields.io/badge/C%2B%2B-23-blue.svg)](https://en.cppreference.com/w/cpp/23)
[![CMake](https://img.shields.io/badge/CMake-3.23+-064F8C.svg)](https://cmake.org/)

*A comprehensive implementation of the C++ asynchronous execution framework based on P2300*

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Examples](#-examples) â€¢ [Documentation](#-documentation) â€¢ [Contributing](#-contributing)

</div>

---

## âš ï¸ Research Project Notice

> This is an **experimental research project** exploring P2300 with member customization points (P2855), non-blocking support (P3669R2), async scopes (P3149), and structured concurrency (P3296). It is **not ready for production use** and is intended for study, experimentation, and academic research purposes only.

---

## ğŸ“– Table of Contents

- [About](#-about)
- [Features](#-features)
- [Technology Stack](#-technology-stack)
- [Quick Start](#-quick-start)
  - [Requirements](#requirements)
  - [Installation](#installation)
  - [Building](#building)
- [Examples](#-examples)
- [Project Structure](#-project-structure)
- [Core Concepts](#-core-concepts)
- [Algorithms & API](#-algorithms--api)
- [Advanced Usage](#-advanced-usage)
- [Testing](#-testing)
- [Performance](#-performance)
- [FAQ](#-faq)
- [Roadmap](#-roadmap)
- [Contributing](#-contributing)
- [License](#-license)
- [References](#-references)
- [Author](#-author)

---

## ğŸ¯ About

**Flow** is a header-only C++23 library that implements the sender/receiver model for structured asynchronous programming. It provides a type-safe, composable, and zero-overhead abstraction for managing asynchronous operations, based on the upcoming C++ standard proposal P2300, with additional support for async scopes (P3149), structured concurrency (P3296), and bulk algorithm improvements (P3481R5).

### What is Flow?

Flow enables you to write asynchronous code that is:
- **Type-safe**: Compile-time verification of sender/receiver connections
- **Composable**: Chain operations using intuitive pipeline syntax
- **Efficient**: Zero-overhead abstractions with aggressive optimization
- **Flexible**: Support for various execution contexts (thread pools, event loops, inline)

### Who is it for?

- ğŸ“ **Researchers** exploring modern C++ asynchronous patterns
- ğŸ“š **Students** learning about sender/receiver execution model
- ğŸ”¬ **Experimenters** investigating P2300 and related proposals
- ğŸ¤ **Contributors** to C++ standardization discussions

### Why Flow?

Traditional callback-based or future-based async programming can be complex and error-prone. Flow provides:
- Clear separation of work description, scheduling, and result handling
- Comprehensive cancellation support with stop tokens and active cancellation
- Structured cancellation and error propagation
- Rich set of composable algorithms including racing operations
- Member function customization points for cleaner syntax
- Async scopes for managing the lifetime of concurrent operations
- Structured concurrency patterns for safe fire-and-forget operations

---

## âœ¨ Features

- **ğŸš€ Modern C++23** - Leverages concepts, requires expressions, and latest language features
- **ğŸ¯ Type-safe** - Compile-time checked sender/receiver connections with full type inference
- **âš¡ Zero-overhead** - Header-only library with excellent optimization potential
- **ğŸ”§ Composable** - Rich set of algorithms for building complex async workflows
- **ğŸ§µ Thread Pool** - Built-in thread pool, run loop, and work-stealing schedulers
- **âš¡ Work-Stealing Scheduler** - High-performance scheduler with per-processor queues and dynamic load balancing
- **ğŸ“Š Algorithms** - `bulk`, `when_all`, `when_any`, `transfer`, `then`, `upon_error`, `upon_stopped`, and more
- **ğŸ¯ Execution Policies** - P3481R5 support with `seq`, `par`, `par_unseq`, `unseq` for bulk operations
- **ğŸ”€ Bulk Variants** - `bulk`, `bulk_chunked`, `bulk_unchunked` for different parallelism patterns
- **ğŸ”„ Retry Mechanisms** - `retry`, `retry_n`, `retry_if`, `retry_with_backoff` for resilient error handling
- **âœ¨ Clean API** - Member function customization points (P2855) for clarity
- **ğŸš« Non-blocking Support** - P3669R2 `try_scheduler` for signal-safe, lock-free operations
- **ğŸ“¦ Async Scopes** - P3149 async scope support with `counting_scope` and `simple_counting_scope`
- **ğŸ¯ Structured Concurrency** - P3296 `let_async_scope` for managing concurrent operations
- **ğŸ›‘ Stop Token Support** - Comprehensive cancellation infrastructure with `inplace_stop_token`
- **ğŸŒ Networking** - Asynchronous network I/O based on P2762R2, P3185R0 (TAPS), and P3482R0
- **ğŸ“¦ C++23 Modules** - Experimental module support via CMake's FILE_SET CXX_MODULES
- **ğŸ§ª Comprehensive Tests** - Extensive test suite with 24+ test categories
- **ğŸ“ Well Documented** - Clear examples and API documentation

---

## ğŸ›  Technology Stack

### Core Technologies

- **Language**: C++23
- **Build System**: CMake 3.23+
- **Threading**: Standard C++ threads (`<thread>`, `<mutex>`, `<condition_variable>`)
- **Testing Framework**: [Boost.UT v2.1.0](https://github.com/boost-ext/ut)
- **Code Quality**:
  - `clang-format` - Code formatting
  - `clang-tidy` - Static analysis

### Dependencies

**Runtime**: None (header-only)
**Build-time**: CMake, C++23 compiler
**Test-time**: Boost.UT (automatically fetched via CMake)

---

## ğŸš€ Quick Start

### Requirements

- **C++23 compatible compiler** (GCC 11+, Clang 14+, or MSVC 2022+)
- **CMake 3.23** or higher
- **Git** (for cloning and FetchContent)

### Installation

#### Option 1: Using CMake FetchContent (Recommended)

Add to your `CMakeLists.txt`:

```cmake
include(FetchContent)

FetchContent_Declare(
    flow
    GIT_REPOSITORY https://github.com/moeryomenko/flow.git
    GIT_TAG        main
)

FetchContent_MakeAvailable(flow)

target_link_libraries(your_target PRIVATE flow::flow)
```

#### Option 2: Manual Installation

```bash
# Clone the repository
git clone https://github.com/moeryomenko/flow.git
cd flow

# Build and install
mkdir build && cd build
cmake ..
cmake --build .
sudo cmake --install .
```

#### Option 3: System-wide Installation

```bash
git clone https://github.com/moeryomenko/flow.git
cd flow
mkdir build && cd build
cmake .. -DFLOW_BUILD_EXAMPLES=OFF -DFLOW_BUILD_TESTS=OFF
sudo cmake --install .
```

Then in your project:

```cmake
find_package(flow REQUIRED)
target_link_libraries(your_target PRIVATE flow::flow)
```

### Building

#### Build with Examples

```bash
mkdir build && cd build
cmake .. -DFLOW_BUILD_EXAMPLES=ON
cmake --build .

# Run examples
./examples/hello_world
./examples/error_handling
./examples/parallel_transform
```

#### Build with Tests

```bash
mkdir build && cd build
cmake .. -DFLOW_BUILD_TESTS=ON
cmake --build .

# Run tests
ctest --output-on-failure
# or
ctest -V
```

#### CMake Build Options

| Option | Default | Description |
|--------|---------|-------------|
| `FLOW_BUILD_EXAMPLES` | `ON` | Build example applications |
| `FLOW_BUILD_TESTS` | `ON` | Build test suite (requires Boost.UT) |
| `FLOW_INSTALL` | `ON` | Generate install target |
| `FLOW_USE_MODULES` | `OFF` | Use C++23 modules (experimental, requires CMake 3.28+) |

#### Using C++ Modules (Experimental)

Flow supports C++23 modules through CMake's experimental `FILE_SET CXX_MODULES` feature:

```bash
# On macOS with Homebrew Clang (Apple Clang doesn't support module scanning)
brew install llvm ninja
export PATH="/opt/homebrew/opt/llvm/bin:$PATH"

mkdir build && cd build
cmake .. -G Ninja \
  -DCMAKE_CXX_COMPILER=clang++ \
  -DCMAKE_CXX_FLAGS="-isysroot $(xcrun --show-sdk-path) -stdlib=libc++" \
  -DCMAKE_EXE_LINKER_FLAGS="-L/opt/homebrew/opt/llvm/lib/c++ -Wl,-rpath,/opt/homebrew/opt/llvm/lib/c++" \
  -DFLOW_USE_MODULES=ON
cmake --build .
```

```bash
# On Linux with Clang 16+
mkdir build && cd build
cmake .. -G Ninja \
  -DCMAKE_CXX_COMPILER=clang++-16 \
  -DFLOW_USE_MODULES=ON
cmake --build .
```

When using modules, import Flow instead of including headers:

```cpp
// Traditional header-only mode
#include <flow/execution.hpp>

// C++ modules mode (when FLOW_USE_MODULES=ON)
import flow;

using namespace flow::execution;

int main() {
    thread_pool pool{4};
    auto work = schedule(pool.get_scheduler()) | then([] {
        return 42;
    });
    auto result = flow::this_thread::sync_wait(work);
    return 0;
}
```

**Requirements for C++ modules:**
- CMake 3.28 or higher
- Ninja build system (recommended)
- Compiler support:
  - Clang 16+ with libc++ (not Apple Clang - use Homebrew llvm)
  - GCC 14+ (experimental)
  - MSVC 19.36+ (Visual Studio 2022 17.6+)

**Important Notes:**
- Apple Clang does not support CMake's module scanning - use `brew install llvm` and set `CMAKE_CXX_COMPILER` to Homebrew Clang
- C++ modules support is experimental and may have limited tooling support
- Module scanning requires additional compile time

---

## ğŸ’¡ Examples

### Hello World

```cpp
#include <flow/execution.hpp>
#include <iostream>

using namespace flow::execution;

int main() {
    // Create a thread pool with 4 threads
    thread_pool pool{4};

    // Build async work chain
    auto work = schedule(pool.get_scheduler())
        | then([] {
            std::cout << "Hello from thread pool!\n";
            return 42;
        })
        | then([](int x) {
            return x * 2;
        });

    // Execute and wait for result
    auto result = flow::this_thread::sync_wait(work);

    if (result) {
        std::cout << "Final result: " << std::get<0>(*result) << '\n';
    }

    return 0;
}
```

### Error Handling

```cpp
#include <flow/execution.hpp>
#include <iostream>
#include <stdexcept>

using namespace flow::execution;

auto risky_computation(int x) {
    return just(x)
        | then([](int val) -> int {
            if (val < 0) {
                throw std::runtime_error("Negative value not allowed!");
            }
            return val * 2;
        })
        | upon_error([](std::exception_ptr ep) -> int {
            try {
                std::rethrow_exception(ep);
            } catch (const std::exception& e) {
                std::cout << "Error caught: " << e.what() << '\n';
                return -1;  // Fallback value
            }
        });
}

int main() {
    auto result = flow::this_thread::sync_wait(risky_computation(5));
    if (result) {
        std::cout << "Result: " << std::get<0>(*result) << '\n';
    }
    return 0;
}
```

### Retry with Backoff

```cpp
#include <flow/execution.hpp>
#include <iostream>
#include <stdexcept>

using namespace flow::execution;

auto unreliable_api_call(int& attempt) {
    return just()
        | then([&attempt] {
            attempt++;
            if (attempt < 3) {
                throw std::runtime_error("Temporary network error");
            }
            return "Success!";
        });
}

int main() {
    thread_pool pool{2};
    int attempt = 0;

    // Retry up to 5 times with exponential backoff
    auto result = flow::this_thread::sync_wait(
        unreliable_api_call(attempt)
        | retry_with_backoff(
            pool.get_scheduler(),
            std::chrono::milliseconds(100),  // initial delay
            std::chrono::milliseconds(2000), // max delay
            2.0,                             // multiplier
            5                                // max attempts
        )
    );

    if (result) {
        std::cout << "Result: " << std::get<0>(*result) << '\n';
        std::cout << "Took " << attempt << " attempts\n";
    }

    return 0;
}
```

### Parallel Transform

```cpp
#include <flow/execution.hpp>
#include <iostream>
#include <vector>
#include <numeric>

using namespace flow::execution;

int main() {
    thread_pool pool{4};

    // Create input data
    std::vector<int> input(1000);
    std::iota(input.begin(), input.end(), 1);  // 1, 2, 3, ..., 1000

    // Parallel computation with execution policy
    auto sender = schedule(pool.get_scheduler())
        | bulk(par, input.size(), [&](size_t i) {
            input[i] = input[i] * input[i];  // square each element
        });

    flow::this_thread::sync_wait(std::move(sender));

    // Sum results
    int sum = std::accumulate(input.begin(), input.end(), 0);
    std::cout << "Sum of squares: " << sum << '\n';

    return 0;
}
```

### Structured Concurrency with Async Scopes

```cpp
#include <flow/execution.hpp>
#include <iostream>

using namespace flow::execution;

int main() {
    thread_pool pool{4};

    auto result = flow::this_thread::sync_wait(
        just() | let_async_scope([&](auto scope_token) {
            // Spawn multiple concurrent tasks
            for (int i = 0; i < 10; ++i) {
                spawn(
                    schedule(pool.get_scheduler()) | then([i] {
                        std::cout << "Task " << i << " executing\n";
                        return i * i;
                    }),
                    scope_token
                );
            }
            // All spawned work automatically completes before continuing
        })
        | then([] {
            std::cout << "All concurrent work completed!\n";
        })
    );

    return 0;
}
```

### Racing Operations with when_any

```cpp
#include <flow/execution.hpp>
#include <iostream>
#include <chrono>
#include <thread>

using namespace flow::execution;

int main() {
    thread_pool pool{4};

    // Create three competing tasks
    auto fast_task = schedule(pool.get_scheduler()) | then([] {
        std::this_thread::sleep_for(std::chrono::milliseconds(10));
        return "fast";
    });

    auto medium_task = schedule(pool.get_scheduler()) | then([] {
        std::this_thread::sleep_for(std::chrono::milliseconds(50));
        return "medium";
    });

    auto slow_task = schedule(pool.get_scheduler()) | then([] {
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        return "slow";
    });

    // Race them - first to complete wins, others are cancelled
    auto race = when_any(std::move(fast_task), std::move(medium_task), std::move(slow_task));
    auto result = flow::this_thread::sync_wait(std::move(race));

    if (result) {
        auto [winner] = *result;
        std::cout << "Winner: " << winner << "\n";
    }

    return 0;
}
```

### Transferring Execution Between Schedulers

```cpp
#include <flow/execution.hpp>
#include <iostream>

using namespace flow::execution;

int main() {
    thread_pool pool1{2};
    thread_pool pool2{2};

    // Start work on pool1, transfer to pool2
    auto work = schedule(pool1.get_scheduler())
        | then([] {
            std::cout << "Computing on pool1\n";
            return 42;
        })
        | transfer(pool2.get_scheduler())  // Move execution to pool2
        | then([](int x) {
            std::cout << "Processing on pool2\n";
            return x * 2;
        });

    auto result = flow::this_thread::sync_wait(work);

    if (result) {
        std::cout << "Final result: " << std::get<0>(*result) << '\n';
    }

    return 0;
}
```

### Asynchronous Networking

```cpp
#include <flow/execution.hpp>
#include <flow/net.hpp>
#include <iostream>

using namespace flow;
using namespace flow::execution;
using namespace flow::net;

int main() {
    // Create I/O context for network operations
    io_context ctx;
    tcp_socket socket(ctx);

    // Configure transport properties (TAPS-inspired)
    auto props = transport_properties::reliable_stream();

    // Open socket
    socket.open(tcp());

    // Async network operations integrate with execution
    std::vector<char> buffer(1024);
    mutable_buffer_sequence buffers;
    buffers.push_back(flow::net::buffer(buffer.data(), buffer.size()));

    auto work = schedule(ctx.get_scheduler())
        | then([&socket, &buffers] {
            // Async receive with sender/receiver
            return async_receive(socket, buffers);
          })
        | then([](std::size_t bytes_received) {
            std::cout << "Received " << bytes_received << " bytes\n";
            return bytes_received;
          })
        | upon_error([](std::error_code ec) {
            std::cout << "Network error: " << ec.message() << '\n';
            return 0;
          });

    auto result = this_thread::sync_wait(std::move(work));

    return 0;
}
```

**See [NETWORKING.md](NETWORKING.md) for comprehensive networking documentation including:**
- I/O context and schedulers
- Socket types (TCP, UDP)
- Async operations (connect, send, receive)
- Buffer management
- TAPS property system
- Security properties
- Error handling patterns

---

## ğŸ“ Project Structure

```
flow/
â”œâ”€â”€ CMakeLists.txt              # Main build configuration
â”œâ”€â”€ LICENSE-APACHE              # Apache 2.0 License
â”œâ”€â”€ LICENSE-MIT                 # MIT License
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ .clang-format               # Code formatting rules
â”œâ”€â”€ .clang-tidy                 # Static analysis configuration
â”‚
â”œâ”€â”€ cmake/
â”‚   â””â”€â”€ flowConfig.cmake.in     # CMake package configuration
â”‚
â”œâ”€â”€ include/
â”‚   â””â”€â”€ flow/
â”‚       â”œâ”€â”€ execution.hpp       # Main header (includes all)
â”‚       â””â”€â”€ execution/
â”‚           â”œâ”€â”€ execution.hpp        # Core concepts and queries
â”‚           â”œâ”€â”€ sender.hpp          # Sender concepts
â”‚           â”œâ”€â”€ receiver.hpp        # Receiver concepts
â”‚           â”œâ”€â”€ scheduler.hpp       # Scheduler concepts
â”‚           â”œâ”€â”€ try_scheduler.hpp   # Non-blocking scheduler support (P3669R2)
â”‚           â”œâ”€â”€ operation_state.hpp # Operation state concepts
â”‚           â”œâ”€â”€ queries.hpp         # Query customization points
â”‚           â”œâ”€â”€ env.hpp             # Execution environments
â”‚           â”œâ”€â”€ completion_signatures.hpp  # Completion signatures
â”‚           â”œâ”€â”€ execution_policy.hpp       # Execution policies (P3481R5)
â”‚           â”œâ”€â”€ factories.hpp       # Sender factories (just, just_error, etc.)
â”‚           â”œâ”€â”€ adaptors.hpp        # Sender adaptors (then, upon_error, etc.)
â”‚           â”œâ”€â”€ algorithms.hpp      # Advanced algorithms (bulk, when_all, when_any, etc.)
â”‚           â”œâ”€â”€ retry.hpp           # Retry mechanisms for error recovery
â”‚           â”œâ”€â”€ async_scope.hpp     # Async scope support (P3149, P3296)
â”‚           â”œâ”€â”€ schedulers.hpp      # Standard scheduler implementations
â”‚           â”œâ”€â”€ lock_free_queue.hpp # Lock-free queue for non-blocking operations
â”‚           â”œâ”€â”€ stop_token.hpp      # Stop token and cancellation support
â”‚           â”œâ”€â”€ sync_wait.hpp       # Synchronous execution utilities
â”‚           â”œâ”€â”€ type_list.hpp       # Type manipulation utilities
â”‚           â””â”€â”€ utils.hpp           # General utilities
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ hello_world.cpp         # Basic usage example
â”‚   â”œâ”€â”€ error_handling.cpp      # Error handling patterns
â”‚   â”œâ”€â”€ parallel_transform.cpp  # Parallel computation example
â”‚   â”œâ”€â”€ try_schedule_example.cpp # Non-blocking operations (P3669R2)
â”‚   â”œâ”€â”€ when_any_example.cpp    # Racing operations example
â”‚   â””â”€â”€ work_stealing_example.cpp # Work-stealing scheduler demonstration
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ CMakeLists.txt
    â”œâ”€â”€ basic_test.cpp                  # Basic functionality tests
    â”œâ”€â”€ concept_tests.cpp               # Concept validation tests
    â”œâ”€â”€ customization_point_tests.cpp   # CPO tests
    â”œâ”€â”€ factory_tests.cpp               # Sender factory tests
    â”œâ”€â”€ adaptor_tests.cpp               # Sender adaptor tests
    â”œâ”€â”€ scheduler_tests.cpp             # Scheduler tests
    â”œâ”€â”€ consumer_tests.cpp              # Consumer (sync_wait) tests
    â”œâ”€â”€ error_handling_tests.cpp        # Error propagation tests
    â”œâ”€â”€ resource_management_tests.cpp   # Resource lifecycle tests
    â”œâ”€â”€ race_condition_tests.cpp        # Thread safety tests
    â”œâ”€â”€ performance_tests.cpp           # Performance benchmarks
    â”œâ”€â”€ integration_tests.cpp           # End-to-end tests
    â”œâ”€â”€ interoperability_tests.cpp      # Cross-component tests
    â”œâ”€â”€ edge_case_tests.cpp             # Corner case handling
    â”œâ”€â”€ module_tests.cpp                # C++ module tests
    â”œâ”€â”€ platform_tests.cpp              # Platform-specific tests
    â”œâ”€â”€ compilation_tests.cpp           # Compile-time tests
    â”œâ”€â”€ advanced_features_test.cpp      # Advanced feature tests
    â”œâ”€â”€ limitations_resolved_test.cpp   # Known issue verification
    â”œâ”€â”€ async_scope_basic_tests.cpp     # Basic async scope tests (P3149)
    â”œâ”€â”€ async_scope_comprehensive_tests.cpp  # Comprehensive scope tests
    â”œâ”€â”€ let_async_scope_tests.cpp       # let_async_scope tests (P3296)
    â”œâ”€â”€ when_any_tests.cpp              # when_any algorithm tests
    â”œâ”€â”€ retry_tests.cpp                 # retry algorithms tests
    â”œâ”€â”€ try_scheduler_tests.cpp         # P3669R2 non-blocking scheduler tests
    â”œâ”€â”€ bulk_policy_tests.cpp           # P3481R5 bulk algorithms with execution policies
    â”œâ”€â”€ work_stealing_scheduler_tests.cpp # Work-stealing scheduler tests
    â”œâ”€â”€ work_stealing_scheduler_concurrency_tests.cpp # Work-stealing concurrency validation
    â””â”€â”€ async_scope_work_stealing_integration_tests.cpp # Async scope + work-stealing integration
```

---

## ğŸ” Core Concepts

### Senders

Senders represent asynchronous work that can be started later. They describe what to compute, not when or where.

```cpp
// Factory senders
auto s1 = just(42);                          // Immediately produces value
auto s2 = just_error(std::runtime_error("")); // Immediately produces error
auto s3 = just_stopped();                     // Immediately signals cancellation

// Scheduled work
auto s4 = schedule(scheduler);                // Work on a scheduler
```

### Receivers

Receivers handle the completion of asynchronous operations through three channels:

```cpp
struct my_receiver {
    using receiver_concept = flow::execution::receiver_t;

    void set_value(int x) && noexcept {
        std::cout << "Success: " << x << '\n';
    }

    void set_error(std::exception_ptr ep) && noexcept {
        std::cout << "Error occurred\n";
    }

    void set_stopped() && noexcept {
        std::cout << "Operation cancelled\n";
    }
};
```

### Schedulers

Schedulers control where and when work executes:

```cpp
// Inline execution (no threading, immediate)
inline_scheduler inline_sch;

// Run loop (single-threaded event loop)
run_loop loop;
auto loop_sch = loop.get_scheduler();

// Thread pool (parallel execution across N threads)
thread_pool pool{8};
auto pool_sch = pool.get_scheduler();

// Work-stealing scheduler (dynamic load balancing)
work_stealing_scheduler ws_sched{8};
auto ws_sch = ws_sched.get_scheduler();
```

### Operation States

Operation states represent running asynchronous operations:

```cpp
auto sender = just(42);
auto receiver = my_receiver{};
auto state = sender.connect(receiver);  // Create operation state
state.start();                          // Begin execution
```

### Async Scopes

Async scopes manage the lifetime of asynchronous operations:

```cpp
// Simple counting scope - tracks operations
simple_counting_scope simple_scope;
auto simple_token = simple_scope.get_token();

// Counting scope - tracks operations with cancellation
counting_scope scope;
auto token = scope.get_token();

// Associate work with scope
auto work = associate(some_sender, token);

// Wait for all work to complete
flow::this_thread::sync_wait(scope.join());
```

### Scope Tokens

Scope tokens control association and wrapping of senders:

```cpp
counting_scope scope;
auto token = scope.get_token();

// Spawn fire-and-forget work
spawn(schedule(pool.get_scheduler()) | then([] {
    // Work that's tracked by scope
}), token);

// Request cancellation
scope.request_stop();
```

---

## ğŸ¨ Algorithms & API

### Sender Factories

Create senders from values or states:

| Function | Description |
|----------|-------------|
| `just(values...)` | Creates sender that immediately produces values |
| `just_error(error)` | Creates sender that immediately produces error |
| `just_stopped()` | Creates sender that immediately signals cancellation |
| `schedule(scheduler)` | Creates sender that schedules work on a scheduler |

### Sender Adaptors

Transform and compose senders:

| Adaptor | Description |
|---------|-------------|
| `then(fn)` | Transform successful values |
| `upon_error(fn)` | Handle errors and recover |
| `upon_stopped(fn)` | Handle cancellation |
| `let_value(fn)` | Chain dependent async operations |
| `let_error(fn)` | Chain error recovery operations |
| `let_async_scope(fn)` | Create async scope for structured concurrency (P3296) |

### Algorithms

Advanced sender operations:

| Algorithm | Description |
|-----------|-------------|
| `bulk(policy, count, fn)` | Execute function for range [0, count) with execution policy |
| `bulk_chunked(policy, count, fn)` | Execute function with begin/end range (basis operation for chunking) |
| `bulk_unchunked(policy, count, fn)` | Execute function per iteration (one agent per iteration) |
| `when_all(senders...)` | Wait for all senders to complete, aggregating results |
| `when_any(senders...)` | Race senders, first to complete wins (with active cancellation) |
| `retry()` | Retry indefinitely on error until success |
| `retry_n(count)` | Retry up to N times on error |
| `retry_if(predicate)` | Retry only if predicate returns true for the error |
| `retry_with_backoff(...)` | Retry with exponential backoff delay |
| `transfer(scheduler)` | Move execution to different scheduler |

#### Execution Policies

Bulk algorithms support standard execution policies to control parallelism and vectorization:

| Policy | Description |
|--------|-------------|
| `seq` | Sequential execution (no parallelism) |
| `par` | Parallel execution allowed |
| `par_unseq` | Parallel and vectorized execution allowed |
| `unseq` | Vectorized execution allowed (no parallelism) |

### Async Scope Operations

Manage concurrent operation lifetimes:

| Operation | Description |
|-----------|-------------|
| `associate(sndr, token)` | Associate sender with scope token |
| `spawn(sndr, token)` | Fire-and-forget work tracked by scope |
| `spawn_future(sndr, token)` | Spawn work and get future sender |
| `scope.join()` | Wait for all associated work to complete |
| `scope.close()` | Prevent new associations |
| `scope.request_stop()` | Request cancellation (counting_scope only) |

### Pipeline Syntax

Chain operations using `operator|`:

```cpp
auto result = schedule(pool.get_scheduler())
    | then([](auto... args) { /* transform */ })
    | upon_error([](auto ep) { /* handle error */ })
    | bulk(100, [](size_t i) { /* parallel work */ });
```

---

## ï¿½ Non-Blocking Operations with P3669R2

Flow implements P3669R2 non-blocking scheduler support, enabling signal-safe and truly lock-free asynchronous operations.

### Why Non-Blocking Operations?

Some execution environments require operations that **never block**:
- **Signal handlers** for asynchronous signals (POSIX signals)
- **Interrupt handlers** in embedded systems
- **Real-time contexts** with strict timing requirements
- **Lock-free algorithms** that must avoid blocking

Regular `schedule()` may block when enqueueing work (e.g., acquiring a mutex on the work queue). P3669R2 introduces `try_schedule()` which either succeeds immediately or signals `would_block_t` error without blocking.

### try_scheduler Concept

Schedulers that support non-blocking operations model the `try_scheduler` concept:

```cpp
using namespace flow::execution;

// Check if scheduler supports try_schedule
static_assert(try_scheduler<decltype(pool.get_scheduler())>);
static_assert(try_scheduler<decltype(loop.get_scheduler())>);
```

Both `run_loop` and `thread_pool` in Flow are `try_scheduler`s.

### try_schedule() Usage

Use `try_schedule()` when blocking is unacceptable:

```cpp
thread_pool pool{4};
auto sch = pool.get_scheduler();

// Non-blocking schedule
auto work = sch.try_schedule()
    | then([] {
        std::cout << "Executing without blocking!\n";
        return 42;
    })
    | upon_error([](would_block_t) {
        std::cout << "Would have blocked - queue was full\n";
        return -1;  // Fallback value
    });

auto result = flow::this_thread::sync_wait(std::move(work));
```

### Signal-Safe Example

Perfect for signal handlers where blocking operations are forbidden:

```cpp
#include <csignal>
#include <flow/execution.hpp>

thread_pool pool{4};

void signal_handler(int signum) {
    // SAFE: try_schedule() is signal-safe
    auto work = pool.get_scheduler().try_schedule()
        | then([signum] {
            // Handle signal asynchronously
            log_signal(signum);
        });

    // Fire and forget (or use async scope)
    spawn(std::move(work), scope_token);
}

int main() {
    std::signal(SIGUSR1, signal_handler);
    // ...
}
```

### Key Guarantees

P3669R2 `try_schedule()` guarantees:

- âœ… **Never blocks** - Returns immediately
- âœ… **Signal-safe** - Can be called from signal handlers
- âœ… **Lock-free** - No mutex acquisition in critical path
- âœ… **No allocation** - Uses fixed-size lock-free ring buffer
- âœ… **Error signaling** - `set_error(would_block_t{})` if queue is full

### Implementation Details

Flow's implementation uses:
- **Lock-free bounded queue** (`lock_free_bounded_queue`) with fixed capacity (1024 items)
- **MPMC ring buffer** with atomic version numbers for ABA problem prevention
- **CAS operations** for wait-free push/pop
- **Exception handling** for `std::function` move construction failures

### When to Use try_schedule

| Scenario | Use try_schedule | Use schedule |
|----------|------------------|--------------|
| Signal handlers | âœ… Yes | âŒ No |
| Interrupt handlers | âœ… Yes | âŒ No |
| Real-time contexts | âœ… Yes | âŒ No |
| Lock-free algorithms | âœ… Yes | âŒ No |
| Normal async work | Either | âœ… Preferred |

### would_block_t Error Type

When `try_schedule()` would block, it signals an error of type `would_block_t`:

```cpp
auto work = sch.try_schedule()
    | then([] { return 42; })
    | upon_error([](would_block_t) {
        // Queue was full, handle gracefully
        return fallback_value;
    })
    | upon_error([](std::exception_ptr ep) {
        // Handle other errors
        return error_value;
    });
```

### Complete Example

See [`examples/try_schedule_example.cpp`](examples/try_schedule_example.cpp) for a comprehensive demonstration of:
- Basic `try_schedule()` usage
- Composing with `then()`
- Multiple concurrent non-blocking operations
- Checking `try_scheduler` support at compile time

---

## ğŸ”„ Retry Mechanisms for Resilient Operations

Flow provides comprehensive retry mechanisms to handle transient failures and build resilient asynchronous applications. All retry algorithms automatically attempt to re-execute failed operations, making your code more robust against temporary errors.

### Basic Retry - Unlimited Attempts

The `retry()` algorithm retries indefinitely until the operation succeeds:

```cpp
#include <flow/execution.hpp>
using namespace flow::execution;

auto flaky_operation = just(42)
    | then([](int x) {
        // Might throw occasionally
        if (random_failure()) {
            throw std::runtime_error("Transient error");
        }
        return x * 2;
    })
    | retry();  // Keep trying until success

auto result = flow::this_thread::sync_wait(std::move(flaky_operation));
```

**Use case**: Operations that should eventually succeed (e.g., connecting to a service that's temporarily down).

### Bounded Retry - Limited Attempts

The `retry_n(count)` algorithm retries up to N times before giving up:

```cpp
auto operation = risky_sender()
    | retry_n(3)  // Try up to 3 times total
    | upon_error([](std::exception_ptr ep) {
        // Handle final failure after all retries exhausted
        return fallback_value;
    });
```

**Use case**: Operations that might fail permanently (e.g., invalid user input, non-existent resources).

### Conditional Retry - Selective Error Handling

The `retry_if(predicate)` algorithm retries only when the predicate returns true:

```cpp
auto selective_retry = fetch_data()
    | retry_if([](std::exception_ptr ep) {
        try {
            std::rethrow_exception(ep);
        } catch (const network_timeout& e) {
            return true;   // Retry on timeout
        } catch (const not_found_error& e) {
            return false;  // Don't retry on 404
        } catch (...) {
            return false;  // Don't retry on other errors
        }
    });
```

**Use case**: Distinguishing between transient failures (network timeout) and permanent failures (resource not found).

### Exponential Backoff - Smart Retry Delays

The `retry_with_backoff(...)` algorithm implements exponential backoff to avoid overwhelming failing services:

```cpp
thread_pool pool{4};

auto resilient_api_call = api_request()
    | retry_with_backoff(
        pool.get_scheduler(),            // Scheduler for delays
        std::chrono::milliseconds(100),  // Initial delay: 100ms
        std::chrono::milliseconds(5000), // Max delay: 5s
        2.0,                             // Multiplier: double each time
        10                               // Max attempts: 10
    );

// Retry delays: 100ms, 200ms, 400ms, 800ms, 1600ms, 3200ms, 5000ms (capped), ...
```

**Use case**: API calls, database connections, or any external service that might be temporarily overloaded.

### Retry Composition

Retry algorithms compose naturally with other sender operations:

```cpp
auto robust_pipeline = schedule(pool.get_scheduler())
    | then([] { return fetch_user_data(); })
    | retry_n(3)                           // Retry fetch up to 3 times
    | then([](auto data) {
        return process_data(data);
    })
    | upon_error([](std::exception_ptr ep) {
        log_error(ep);
        return default_result;
    });
```

### Key Features

- **ğŸ” Automatic Retry**: Transparent retry logic without manual loops
- **ğŸ¯ Selective Recovery**: Choose which errors to retry
- **â±ï¸ Smart Backoff**: Exponential delays prevent service overload
- **ğŸ”— Composable**: Works seamlessly with other sender operations
- **ğŸ›‘ Cancellation-Aware**: Respects stop tokens and cancellation requests
- **ğŸ“Š Type-Safe**: Preserves completion signatures through retry chain

### Best Practices

1. **Use `retry_n()` by default** to prevent infinite loops
2. **Implement backoff for external services** to be a good citizen
3. **Use `retry_if()` for selective retry** based on error types
4. **Log retry attempts** for debugging and monitoring
5. **Set reasonable limits** on attempts and delays
6. **Combine with timeouts** to prevent hanging operations

---

## âš–ï¸ Work-Stealing Scheduler for Dynamic Load Balancing

Flow provides a high-performance work-stealing scheduler inspired by Go's runtime. This scheduler optimizes CPU utilization through dynamic load balancing, making it ideal for workloads with varying task durations.

### Why Work-Stealing?

Traditional thread pools use a single shared queue, which can become a bottleneck under high contention. The work-stealing scheduler addresses this by:

- **Per-processor local queues**: Each worker thread maintains its own task queue, minimizing lock contention
- **Dynamic load balancing**: Idle workers steal work from busy workers, ensuring optimal CPU utilization
- **Minimal contention**: Short time slices and minimal locking reduce synchronization overhead
- **Better cache locality**: Workers primarily execute their own tasks, improving cache hit rates

### Architecture

The work-stealing scheduler implements a G-P-M model (borrowed from Go):

- **G (Goroutine/Task)**: Lightweight task abstraction representing a unit of work
- **P (Processor)**: Logical processor with a local run queue (up to 256 tasks)
- **M (Machine)**: OS thread that executes tasks from a processor

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Global Run Queue                   â”‚
â”‚         (overflow & load balancing)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                    â†‘
         â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚   P0    â”‚          â”‚   P1    â”‚
    â”‚  Local  â”‚  steal   â”‚  Local  â”‚
    â”‚  Queue  â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Queue  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚   M0    â”‚          â”‚   M1    â”‚
    â”‚ Worker  â”‚          â”‚ Worker  â”‚
    â”‚ Thread  â”‚          â”‚ Thread  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Basic Usage

```cpp
#include <flow/execution.hpp>
using namespace flow::execution;

int main() {
    // Create scheduler with hardware concurrency threads
    work_stealing_scheduler sched;
    auto scheduler = sched.get_scheduler();

    // Or specify thread count
    work_stealing_scheduler sched_8(8);  // 8 worker threads

    // Use like any other scheduler
    auto work = schedule(scheduler) | then([] {
        return compute_result();
    });

    auto result = flow::this_thread::sync_wait(work);
    return 0;
}
```

### Performance Monitoring with Statistics

The work-stealing scheduler provides detailed statistics for monitoring and debugging:

```cpp
work_stealing_scheduler sched(4);
auto scheduler = sched.get_scheduler();

// Execute some work...
for (int i = 0; i < 100; ++i) {
    auto work = schedule(scheduler) | then([i] {
        return compute(i);
    });
    flow::this_thread::sync_wait(work);
}

// Inspect per-processor statistics
for (size_t i = 0; i < 4; ++i) {
    auto stats = sched.get_stats(i);
    std::cout << "Processor " << i << ":\n"
              << "  Tasks executed: " << stats.tasks_executed << "\n"
              << "  Local queue pops: " << stats.local_queue_pops << "\n"
              << "  Global queue pops: " << stats.global_queue_pops << "\n"
              << "  Steal attempts: " << stats.steals_attempted << "\n"
              << "  Successful steals: " << stats.steals_succeeded << "\n";
}
```

### When to Use Work-Stealing Scheduler

| Scenario | Work-Stealing Scheduler | Thread Pool |
|----------|------------------------|-------------|
| Heterogeneous task durations | âœ… Excellent | âš ï¸ May underutilize CPUs |
| High task submission rate | âœ… Low contention | âš ï¸ Single queue bottleneck |
| CPU-bound workloads | âœ… Optimal distribution | âœ… Also good |
| I/O-bound workloads | âœ… Good | âœ… Good |
| Fine-grained parallelism | âœ… Excellent | âš ï¸ More overhead |
| Simple uniform tasks | âœ… Good | âœ… Simpler |

### Work-Stealing Algorithm

1. **Local queue first** (FIFO): Worker attempts to pop from its own local queue
2. **Global queue check** (every 61 tasks): Periodically checks global queue for fairness
3. **Work stealing** (on idle): Randomly selects a victim processor and steals from the back of their queue
4. **Wait with timeout**: If no work found, waits briefly before rechecking

This strategy balances:
- **Cache locality**: Workers prefer their own tasks
- **Fairness**: Global queue prevents starvation
- **Load balancing**: Stealing redistributes work from busy to idle workers

### Integration with Async Scopes

The work-stealing scheduler integrates seamlessly with async scopes for structured concurrency:

```cpp
work_stealing_scheduler sched(4);
auto scheduler = sched.get_scheduler();

auto result = flow::this_thread::sync_wait(
    just() | let_async_scope([&](auto scope_token) {
        // Spawn multiple tasks that benefit from work stealing
        for (int i = 0; i < 100; ++i) {
            spawn(
                schedule(scheduler) | then([i] {
                    return process_item(i);
                }),
                scope_token
            );
        }
        // Work is automatically distributed and balanced
    })
);
```

### try_schedule Support

Like other Flow schedulers, the work-stealing scheduler supports non-blocking operations:

```cpp
work_stealing_scheduler sched(4);
auto scheduler = sched.get_scheduler();

static_assert(try_scheduler<decltype(scheduler)>);

auto work = try_schedule(scheduler)
    | then([] { return 42; })
    | upon_error([](would_block_t) {
        return -1;  // Handle queue full
    });
```

### Performance Characteristics

**Strengths:**
- âœ… **Low contention**: O(1) local queue operations without locks (try-lock only)
- âœ… **Dynamic balancing**: Automatic load distribution across workers
- âœ… **Cache efficiency**: Workers mostly execute their own tasks
- âœ… **Scalability**: Performs well with increasing thread counts

**Trade-offs:**
- âš ï¸ **Memory overhead**: Each processor maintains a separate queue
- âš ï¸ **Stealing cost**: Cross-processor stealing has some overhead
- âš ï¸ **Complexity**: More complex than simple thread pool

### Complete Example

See [`examples/work_stealing_example.cpp`](examples/work_stealing_example.cpp) for comprehensive demonstrations including:
- Basic usage patterns
- Performance comparison with standard thread pool
- Statistics monitoring and analysis
- Load distribution visualization
- Heterogeneous task distribution

### Key Features

- **ğŸ”„ Dynamic Load Balancing**: Automatic work redistribution
- **ğŸ“Š Performance Monitoring**: Detailed per-processor statistics
- **ğŸ”“ Low Contention**: Per-processor local queues
- **ğŸš€ High Throughput**: Minimal lock acquisition in hot paths
- **ğŸ”— Fully Compatible**: Works with all Flow algorithms and adaptors
- **ğŸ›‘ Cancellation Support**: Respects stop tokens and async scopes
- **ğŸ“ˆ Scalable**: Efficient with high worker thread counts

---

## ğŸ›‘ Cancellation & Stop Tokens

Flow provides comprehensive support for cooperative cancellation through stop tokens, enabling graceful termination of asynchronous operations.

### Stop Token Types

Flow supports multiple stop token types:

```cpp
// Standard C++ stop token
std::stop_source source;
std::stop_token token = source.get_token();

// Inplace stop token (no allocation, lightweight)
flow::execution::inplace_stop_source source;
flow::execution::inplace_stop_token token = source.get_token();
```

### Query Stop Tokens from Environment

Senders can query stop tokens from their execution environment:

```cpp
auto work = schedule(pool.get_scheduler()) | then([](auto) {
    auto env = flow::execution::get_env(/* receiver */);
    auto token = flow::execution::get_stop_token(env);

    while (!token.stop_requested()) {
        // Do work that respects cancellation
    }
});
```

### Stop Callbacks

Register callbacks to be invoked when cancellation is requested:

```cpp
flow::execution::inplace_stop_source source;
auto token = source.get_token();

// Register callback
flow::execution::inplace_stop_callback callback(token, [] {
    std::cout << "Cancellation requested!\n";
});

source.request_stop();  // Callback is invoked
```

### when_any Active Cancellation

The `when_any` algorithm demonstrates active cancellation - when the first operation completes, remaining operations are automatically cancelled:

```cpp
auto fast = schedule(pool.get_scheduler()) | then([] { return 1; });
auto slow = schedule(pool.get_scheduler()) | then([] {
    // This will be cancelled when fast completes
    std::this_thread::sleep_for(std::chrono::seconds(10));
    return 2;
});

auto race = when_any(std::move(fast), std::move(slow));
auto result = flow::this_thread::sync_wait(std::move(race));
// slow is automatically cancelled, doesn't wait 10 seconds
```

### Environment with Stop Token

Create execution environments with custom stop tokens:

```cpp
flow::execution::inplace_stop_source source;
auto env = flow::execution::make_env_with_stop_token(
    source.get_token(),
    flow::execution::empty_env{}
);
```

---

## ğŸ­ Async Scopes & Structured Concurrency

Flow provides comprehensive support for managing the lifetime of asynchronous operations through async scopes (P3149) and structured concurrency patterns (P3296).

### Simple Counting Scope

Basic scope that tracks active operations:

```cpp
#include <flow/execution.hpp>
using namespace flow::execution;

simple_counting_scope scope;
auto token = scope.get_token();

// Associate work with scope
auto work = associate(schedule(pool.get_scheduler()) | then([] {
    std::cout << "Work in scope\n";
    return 42;
}), token);

flow::this_thread::sync_wait(work);

// Wait for all work to complete
flow::this_thread::sync_wait(scope.join());
```

### Counting Scope

Advanced scope with stop token support for cancellation:

```cpp
counting_scope scope;
auto token = scope.get_token();

// Spawn fire-and-forget work
spawn(schedule(pool.get_scheduler()) | then([] {
    std::cout << "Background work\n";
}), token);

// Request cancellation of all work
scope.request_stop();

// Wait for cleanup
flow::this_thread::sync_wait(scope.join());
```

### let_async_scope - Structured Concurrency

The `let_async_scope` adaptor provides a safe way to spawn concurrent work that automatically waits for completion:

```cpp
auto result = flow::this_thread::sync_wait(
    just(42) | let_async_scope([&](auto scope_token) {
        // Spawn multiple concurrent operations
        spawn(schedule(pool.get_scheduler()) | then([](int x) {
            std::cout << "Processing " << x << "\n";
        }), scope_token);

        spawn(schedule(pool.get_scheduler()) | then([](int x) {
            std::cout << "Also processing " << x << "\n";
        }), scope_token);

        // Scope automatically waits for all spawned work
    })
);
```

### Spawn Future

Create futures from spawned work:

```cpp
counting_scope scope;
auto token = scope.get_token();

auto future = spawn_future(
    schedule(pool.get_scheduler()) | then([] {
        return 42;
    }),
    token
);

// Use future as a sender
auto result = flow::this_thread::sync_wait(future);

flow::this_thread::sync_wait(scope.join());
```

### Key Features

- **Lifetime Management**: Automatically track async operations
- **Structured Cancellation**: Stop token propagation
- **Error Handling**: First error stops all work in `let_async_scope`
- **Type Safety**: Compile-time checked scope associations
- **Fire-and-Forget**: Safe `spawn` with automatic cleanup

---

## ğŸ”§ Advanced Usage

### Custom Scheduler

Implement your own execution context:

```cpp
class my_scheduler {
public:
    using scheduler_concept = flow::execution::scheduler_t;

    auto schedule() const {
        return my_sender{/* ... */};
    }

    auto query(flow::execution::get_forward_progress_guarantee_t) const {
        return flow::execution::forward_progress_guarantee::parallel;
    }

    bool operator==(const my_scheduler&) const = default;
};
```

### Custom Sender

Create specialized async operations:

```cpp
struct my_sender {
    using sender_concept = flow::execution::sender_t;

    template<class Env>
    auto get_completion_signatures(Env&&) const {
        return flow::execution::completion_signatures<
            flow::execution::set_value_t(int)
        >{};
    }

    template<flow::execution::receiver R>
    auto connect(this my_sender, R&& r) {
        return my_operation{std::forward<R>(r)};
    }
};
```

### Cancellation Support

Implement cancellation-aware operations:

```cpp
auto cancellable_work = schedule(scheduler)
    | then([](stop_token token) {
        while (!token.stop_requested()) {
            // Do work
        }
        return result;
    });
```

---

## ğŸ§ª Testing

Flow includes a comprehensive test suite with 29+ test categories:

```bash
# Build with tests
cmake .. -DFLOW_BUILD_TESTS=ON
cmake --build .

# Run all tests
ctest

# Run specific test
./tests/scheduler_tests
./tests/async_scope_basic_tests
./tests/let_async_scope_tests
./tests/work_stealing_scheduler_tests

# Run tests with verbose output
ctest -V
```

### Test Categories

- **Basic Tests**: Core functionality validation
- **Concept Tests**: C++23 concept compliance
- **Customization Point Tests**: CPO behavior verification
- **Factory Tests**: Sender factory correctness
- **Adaptor Tests**: Sender adaptor chains
- **Scheduler Tests**: Execution context behavior
- **Consumer Tests**: `sync_wait` and blocking operations
- **Error Handling Tests**: Exception propagation
- **Resource Management Tests**: RAII and lifecycle
- **Race Condition Tests**: Thread safety validation
- **Performance Tests**: Benchmarks and optimization checks
- **Integration Tests**: End-to-end scenarios
- **Edge Case Tests**: Corner cases and boundaries
- **Platform Tests**: OS-specific behavior
- **Async Scope Tests**: P3149 scope functionality
- **let_async_scope Tests**: P3296 structured concurrency
- **when_any Tests**: Racing operations and active cancellation
- **Retry Tests**: Retry mechanisms and error recovery
- **Bulk Policy Tests**: P3481R5 execution policies and bulk variants
- **Work-Stealing Scheduler Tests**: Work-stealing behavior and statistics
- **Work-Stealing Concurrency Tests**: Thread safety and memory ordering validation
- **Async Scope Work-Stealing Integration**: Integration between async scopes and work-stealing scheduler

---

## âš¡ Performance

Flow is designed for **zero-overhead abstraction**:

### Design Principles

- âœ… **Header-only template library** - No runtime library overhead
- âœ… **Aggressive inlining** - All core operations marked `inline`
- âœ… **Move-only semantics** - Where appropriate to avoid copies
- âœ… **Minimal dynamic allocation** - Stack-based operation states when possible
- âœ… **Lock-free algorithms** - Where applicable (P3669 support)
- âœ… **Compile-time type checking** - No runtime type overhead

### Optimization Tips

1. **Use `-O3` and LTO** for production builds
2. **Profile-guided optimization (PGO)** can help with hot paths
3. **Avoid unnecessary `sync_wait`** - prefer async composition
4. **Reuse schedulers** - thread pool creation is expensive
5. **Batch operations** with `bulk` for better cache locality

---

## â“ FAQ

### Is Flow production-ready?

**No.** Flow is an experimental research project. It should **not** be used in production systems or critical applications. Use it for:
- Learning and education
- Experimenting with P2300 concepts
- Academic research
- Contributing to standardization discussions

### Why not use existing async libraries?

Flow specifically explores:
- **P2855**: Member customization points (different from tag_invoke)
- **P3669R2**: Non-blocking scheduler support with `try_scheduler` and lock-free queues
- **P3149**: Async scope support for lifetime management
- **P3296**: Structured concurrency with `let_async_scope`
- **Modern C++23**: Latest language features

If you need a production-ready solution, consider:
- [libunifex](https://github.com/facebookexperimental/libunifex) (Meta/Facebook)
- [stdexec](https://github.com/NVIDIA/stdexec) (NVIDIA)

### Can I use this with C++20?

**No.** Flow requires C++23 features:
- Deducing `this`
- Extended `constexpr`
- Concepts improvements

### How does Flow differ from futures/promises?

| Feature | Futures/Promises | Flow (Senders/Receivers) |
|---------|------------------|--------------------------|
| Composition | Limited chaining | Rich algorithm library |
| Type Safety | Runtime errors possible | Compile-time checked |
| Cancellation | Often missing | First-class support |
| Custom execution | Difficult | Scheduler abstraction |
| Error handling | Exception-based | Three-channel (value/error/stop) |
| Lazy execution | Eager by default | Lazy by design |

### What about coroutines?

Flow senders can be used **with** coroutines! The sender/receiver model complements coroutines:
- Senders for pipeline-style async chains
- Coroutines for sequential async control flow
- `co_await sender` integration possible

### How can I contribute?

See [Contributing](#-contributing) section below!

---

## ğŸ—º Roadmap

### Current Status (v1.0.0)

- âœ… Core sender/receiver model
- âœ… Standard schedulers (inline, run_loop, thread_pool)
- âœ… Work-stealing scheduler with dynamic load balancing
- âœ… Essential algorithms (then, upon_error, bulk, when_all, when_any)
- âœ… Bulk algorithms with execution policies (P3481R5) - `bulk`, `bulk_chunked`, `bulk_unchunked`
- âœ… Execution policy support - `seq`, `par`, `par_unseq`, `unseq`
- âœ… Stop token support (inplace_stop_token, inplace_stop_source)
- âœ… Active cancellation in when_any
- âœ… Non-blocking operations (P3669R2) - `try_scheduler`, `try_schedule`
- âœ… Lock-free bounded queue for signal-safe operations
- âœ… Async scopes (P3149) - `simple_counting_scope`, `counting_scope`
- âœ… Structured concurrency (P3296) - `let_async_scope`
- âœ… Spawn operations (`spawn`, `spawn_future`)
- âœ… Retry mechanisms - `retry`, `retry_n`, `retry_if`, `retry_with_backoff`
- âœ… C++23 Modules - Experimental support via CMake FILE_SET CXX_MODULES
- âœ… Comprehensive test suite (29+ tests, including work-stealing scheduler tests)
- âœ… Example programs

### Future Explorations

- ğŸ”„ **More algorithms** - `repeat`, `split`, `timeout`, etc.
- ğŸ”„ **Coroutine integration** - `co_await` sender support
- ğŸ”„ **I/O schedulers** - Async I/O primitives
- ğŸ”„ **Timer support** - Scheduled/delayed execution
- ğŸ”„ **Performance benchmarks** - Comparison with other implementations
- ğŸ”„ **Enhanced error handling** - Better error aggregation in scopes

**Note**: As a research project, priorities may shift based on standardization developments.

---

## ğŸ¤ Contributing

Contributions are welcome! This is a research project, so experimental ideas are encouraged.

### How to Contribute

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes using [Conventional Commits](https://www.conventionalcommits.org/)
4. **Push** to your branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Contribution Ideas

- ğŸ› **Bug reports** - Found an issue? Let us know!
- ğŸ’¡ **Feature requests** - Ideas for new algorithms or improvements
- ğŸ“ **Documentation** - Improve examples, comments, or README
- ğŸ§ª **Tests** - Add test cases or improve coverage
- âš¡ **Performance** - Optimization suggestions or benchmarks
- ğŸ”¬ **Research** - Share findings from using Flow in your studies

### Code Style

- Follow the `.clang-format` configuration
- Run `clang-tidy` before submitting
- Add tests for new features
- Update documentation as needed

```bash
# Format code
cmake --build build --target format

# Run linter
cmake --build build --target lint

# Auto-fix lint issues
cmake --build build --target lint-fix
```

### Discussion

- **GitHub Issues**: [Report bugs or ask questions](https://github.com/moeryomenko/flow/issues)
- **GitHub Discussions**: [Share ideas and research findings](https://github.com/moeryomenko/flow/discussions)

**Note**: As an experimental project, breaking changes may occur. We'll try to communicate these clearly.

---

## ğŸ“„ License

This project is dual-licensed under your choice of:

- **MIT License** - See [LICENSE-MIT](./LICENSE-MIT)
- **Apache License 2.0** - See [LICENSE-APACHE](./LICENSE-APACHE)

You may use this project under the terms of either license.

---

## ğŸ“š References

### C++ Standards Proposals

- [P2300R10: `std::execution`](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p2300r10.html) - Core sender/receiver model
- [P2855: Member customization points](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p2855r1.html) - Member function CPOs
- [P3149: Async scopes](https://open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3149r11.html) - Lifetime management for async operations
- [P3296: `let_async_scope`](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3296r4.html) - Structured concurrency patterns
- [P3481R5: `bulk` improvements](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3481r5.html) - Execution policies and bulk algorithm variants
- [P3669R2: Non-blocking support](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3669r2.html) - Signal-safe `try_scheduler` and `try_schedule`

### Related Projects

- [libunifex](https://github.com/facebookexperimental/libunifex) - Meta's production implementation
- [stdexec](https://github.com/NVIDIA/stdexec) - NVIDIA's reference implementation
- [Boost.Asio](https://www.boost.org/doc/libs/1_82_0/doc/html/boost_asio.html) - Mature async I/O library

### Learning Resources

- [Structured Concurrency](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/) - Foundational concepts
- [C++23 Features](https://en.cppreference.com/w/cpp/23) - Language features used

---

## ğŸ‘¨â€ğŸ’» Author

**Maxim Eryomenko**

- GitHub: [@moeryomenko](https://github.com/moeryomenko)
- Email: maxim_eryomenko@rambler.ru

---

## ğŸ™ Acknowledgments

- The C++ Standards Committee for P2300, P2855, and P3669
- [Boost.UT](https://github.com/boost-ext/ut) for the excellent testing framework
- The C++ community for feedback and discussions

---

<div align="center">

**â­ Star this repo if you find it useful for your research or learning!**

Made with â¤ï¸ for the C++ community

</div>
